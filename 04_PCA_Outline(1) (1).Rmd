---
title: "BOO 2025 - Example Analysis"
subtitle: "Script 4: PCA - Questions"
date: "`r Sys.Date()`" 
author: 
 Hala Saab
output:
  html_document:
    code_download: true
    theme: united
    highlight: tango
    df_print: paged
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

***

> In this script, you will perform principal component analysis (PCA) to further explore patterns in the project data.

***

# Setup

## Clean

As before, we perform several setup steps at the start of the script to ensure our work is reproducible and clear. 

**Exercise 1: Download the R markdown template, clean your environment, and set the following variables:**

* `root_dir` - project folder
* `cpm_path` - location of the `cpmData` object
* `metadata_path` - location of the `metaData` object

```{r script4}

 rm(list = ls())

```

```{r clean}
cpm_path <- file.path("Output/cpm_store.Rdata")
metadata_path <- file.path("Output/metadata_store.Rdata")
```

***

## Packages

Two new packages are introduced in this script:

* `ComplexHeatmap` allows the drawing and annotation of heatmaps in R
* `circlize` allows for the drawing of circular plots, but is also used by `ComplexHeatmap` for colour functions like `colorRamp2()`

**Exercise 2: Load these packages alongside `tidyverse` into your R environment:**

```{r load-packages, warning=F, message=F}

if (!require("BiocManager", quietly = TRUE))     
    install.packages("BiocManager") 
BiocManager::install("ComplexHeatmap")
```

```{r}
library(tidyverse)
library(ComplexHeatmap)
library(circlize)
```

***

## Load data

**Exercise 3: Load the CPM and metadata into your R environment:**

```{r load-data, warning=FALSE, message=FALSE}


metadata  <- read.csv("output/metadata_store.csv")

cpmData  <- read.csv("output/cpm_store.csv")

```

***

# PCA

## Calculate

In high dimensional data (such as this data with around 10,000 genes), principal components (PCs) can be calculated and used to explore patterns. PCs can be thought of as a new axis in the data that captures the most variance possible in the fewest variables. 

**Exercise 4: Use the `prcomp()` function to calculate PCs of the `cpmData`:**

```{r pca-calc}

#  Perform PCA, prcomp expects samples as rows and variables (genes) as columns.

        
# Assuming 'cpmData' is loaded from Exercise 3.

# 1. Prepare cpmData: Ensure it's a numeric matrix with gene/probe names as rownames.
#    Handle NAs/negatives.

if (!exists("cpmData")) {
    stop("cpmData object not found. Please ensure it was loaded correctly in Exercise 3.")
}

cpmData_numeric_values <- NULL
cpmData_rownames <- NULL

if (is.data.frame(cpmData)) {
    # Case 1: First column is character (likely gene/probe IDs), rest should be numeric samples
    if (is.character(cpmData[[1]])) {
        # print("Processing cpmData: Assuming first column is gene/probe IDs.")
        cpmData_rownames <- cpmData[[1]]
        # Attempt to convert all other columns to numeric
        # Using lapply to apply as.numeric to each of the data columns
        temp_list_of_cols <- lapply(cpmData[, -1, drop = FALSE], function(col) {
            if(is.factor(col)) as.numeric(as.character(col)) else as.numeric(col)
        })
        numeric_cols_df <- as.data.frame(temp_list_of_cols)
        
        # Explicitly check if all resulting columns in numeric_cols_df are indeed numeric
        are_all_numeric_after_lapply <- sapply(numeric_cols_df, is.numeric)
        if (!all(are_all_numeric_after_lapply)) {
            non_numeric_identified <- colnames(numeric_cols_df)[!are_all_numeric_after_lapply]
            stop(paste("Failed to convert all sample columns in cpmData to numeric. Problematic columns after lapply:", paste(non_numeric_identified, collapse=", ")))
        }
        
        # Convert to numeric matrix; data.matrix should work if all columns in numeric_cols_df are truly numeric.
        cpmData_numeric_values <- data.matrix(numeric_cols_df) 
        
        # Final check: ensure the resulting matrix is numeric
        if (!is.numeric(cpmData_numeric_values)) {
            # This would be very unexpected if the previous check passed.
            # It might indicate that data.matrix itself produced a character matrix due to some subtle issue.
            # One last attempt: force mode. This is a bit of a brute-force approach.
            # print("Warning: data.matrix() did not result in a numeric matrix. Forcing mode to numeric.")
            original_dimnames_temp <- dimnames(cpmData_numeric_values) # Preserve dimnames
            mode(cpmData_numeric_values) <- "numeric"
            dimnames(cpmData_numeric_values) <- original_dimnames_temp # Restore dimnames
            if (!is.numeric(cpmData_numeric_values)) {
                 stop("CRITICAL: cpmData_numeric_values could not be converted to a numeric matrix even after forcing mode.")
            }
        }
        rownames(cpmData_numeric_values) <- cpmData_rownames
    } 
    # Case 2: All columns are already numeric, gene/probe IDs might be rownames
    else if (all(sapply(cpmData, is.numeric))) {
        # print("Processing cpmData: Assuming all columns are numeric samples, and gene/probe IDs are rownames.")
        if (!(is.character(rownames(cpmData)[1]) && !grepl("^[0-9]+$", rownames(cpmData)[1]))) {
             stop("cpmData is all numeric but has default integer row names or missing row names. Ensure gene/probe names are set as row names before PCA.")
        }
        cpmData_numeric_values <- data.matrix(cpmData) # Already numeric, ensure it's a matrix
    } 
    # Case 3: Unexpected structure
    else {
        stop("cpmData is a data frame with an unexpected structure (e.g., mixed non-numeric sample columns or multiple leading character columns). Please inspect cpmData to have one leading gene/probe ID column (or gene/probe IDs as rownames) and all subsequent sample columns as numeric.")
    }
} else if (is.matrix(cpmData)) {
    # print("Processing cpmData: It's already a matrix.")
    if (!is.numeric(cpmData)) {
        # print("cpmData matrix is not numeric. Attempting to convert its mode.")
        original_dimnames <- dimnames(cpmData)
        cpmData_numeric_values <- matrix(as.numeric(cpmData), nrow=nrow(cpmData), ncol=ncol(cpmData))
        dimnames(cpmData_numeric_values) <- original_dimnames
        if (!is.numeric(cpmData_numeric_values)) {
             stop("Failed to convert character matrix cpmData to a numeric matrix.")
        }
    } else {
        cpmData_numeric_values <- cpmData # Already a numeric matrix
    }
} else {
    stop("cpmData is not a data.frame or matrix. Please check the loaded object.")
}

# At this point, cpmData_numeric_values should be a purely numeric matrix 
# with gene/probe names as rownames.

if (any(is.na(cpmData_numeric_values))) {
    cpmData_numeric_values[is.na(cpmData_numeric_values)] <- 0 
}
if (any(cpmData_numeric_values < 0)) {
    cpmData_numeric_values[cpmData_numeric_values < 0] <- 0 
}
# Log transformation step was removed as per a previous request.
# PCA will be performed on cpmData_numeric_values directly.

# 2. Perform PCA
pca_results <- NULL 
tryCatch({
  data_for_prcomp <- t(cpmData_numeric_values) # Transpose: samples become rows, genes become columns
  
  col_variances <- apply(data_for_prcomp, 2, var, na.rm = TRUE)
  genes_with_zero_variance_idx <- which(col_variances == 0 | is.na(col_variances)) # Also check for NA variances

  if (length(genes_with_zero_variance_idx) > 0) {
      # genes_with_zero_variance <- colnames(data_for_prcomp)[genes_with_zero_variance_idx]
      # print(paste("Removing", length(genes_with_zero_variance), "genes with zero or NA variance before PCA. Examples:", head(genes_with_zero_variance, 3)))
      data_for_prcomp <- data_for_prcomp[, -genes_with_zero_variance_idx, drop = FALSE]
      if (ncol(data_for_prcomp) == 0) {
          stop("All genes had zero or NA variance after filtering. Cannot perform PCA.")
      }
  }
  
  pcs <- prcomp(data_for_prcomp, center = TRUE, scale. = TRUE)
  
}, error = function(e) {
  print(paste("ERROR during prcomp(): ", conditionMessage(e)))
})

# if(!is.null(pca_results)) {
#   print("PCA successful. Summary of first few PCs:")
#   print(summary(pca_results)$importance[,1:min(5, ncol(summary(pca_results)$importance))])
# } else {
#   print("PCA failed. 'pca_results' is NULL.")
# }



#The tol argument in prcomp() is a threshold used to determine which principal components are kept

```

Tolerance (or `tol`) can be adjusted to create more or fewer PCs, where a lower tolerance generates a higher number. If this argument is not set, the PCs calculated will capture the full variability of the CPM data.

***

## Variance explained

**Exercise 5: Use the output of `prcomp()` to explore your PCs and calculate the variance in CPM values that they explain:**

<details>
  <summary><strong>Hint</strong></summary>

  Variance explained is the SD squared divided by the sum of the variance for all PCs. 

</details>

```{r pc-summ}
# insure pca_results exists and is not NULL (i.e., PCA ran successfully)
summary(pcs)
# Calculate variance explained
var_explained =
  data.frame(PC = 1:nrow(metadata),
             var_explained = round(pcs$sdev^2 / sum(pcs$sdev^2), 3))

# Inspect
var_explained

```


***

## Screeplot

A screeplot can be used to visualize how each subsequent PC captures less and less of the total variance in the data.

**Exercise 6: Plot the top 20 calculated PCs against the variance they explain to generate a screeplot:**


```{r screeplot}
# n
var_explained %>% 
  # Subset variance explained to include only the first 20 PCs
  filter(PC <= 20) %>%
  # Plot PC against variance explained
  ggplot(aes(x = PC, y = var_explained)) +  
  # Draw a line between points
  geom_line(color = "grey40") + 
  # Draw points for each PC
  geom_point(color = "grey40", fill = 'lightblue', shape = 21, size = 3) +
  # Label x axis at integer values between 1 and 20
  scale_x_continuous(breaks = c(seq(1,20))) + 
  xlab("Principal Component") + 
  ylab("Proportion of variance explained") +
  ggtitle("Screeplot of the first 20 PCs") +
  theme_bw() +
  theme(axis.text.x = element_text(size = 8))


```

***

## Relation to known variables

By investigating how PCs correlate with known variables, we can assess how much each factor impacts expression. 

**Exercise 7: Add the PCs that explain more than 1% variance in CPM values to the metaData for further investigation:**

```{r add-pc}
# Add first 10 PCs to metadata
metaData <- cbind(metadata, pcs$x[,1:10])


```

***

Correlations between known factors and PCs can be calculated using the `cor()` function, which was used for the replicate correlation in the sample QC.

**Exercise 8: Generate a matrix of correlations between PCs explaining over 1% of CPM variance and known factors**

<details>
  <summary><strong>Hint</strong></summary>

  Variables that have a standard deviation above 0 will have a correlation of NA, so you may want to remove these.

</details>

```{r plot-vars, warning=F, message=F}
# Calculate the SD of each column in metadata
plot_vars <- apply(metaData, 2, function(x) sd(as.numeric(factor(x)), na.rm=T))

# Save column names that vary (SD is not NA or 0)
plot_vars <- names(plot_vars[!plot_vars %in% c(NA, 0)])

# Remove PC column names
plot_vars <- plot_vars[!grepl("PC", plot_vars)]

# Inspect
plot_vars
# Subset metadata to include only these columns
heatmap_df <- metaData %>% 
  select(any_of(plot_vars))
# Convert all variables to numeric values
heatmap_df <- apply(heatmap_df, 2, function(x) as.numeric(factor(x)))

# Calculate correlations between the first 10 PCs and these numeric variables scaled
cxy <- round(cor(pcs$x[,1:10], scale(heatmap_df), 
                 use = "pairwise.complete.obs"), 2) 

# Inspect
as.data.frame(cxy)

# Values Range: The numbers in the table range from -1 to 1.

#A value close to 1 means a strong positive linear correlation (e.g., as the factor's value increases, the PC score tends to increase).
#A value close to -1 means a strong negative linear correlation (e.g., as the factor's value increases, the PC score tends to decrease).
# A value close to 0 means a weak or no linear correlation between that factor and that PC.

#PC2: Shows a moderate negative correlation of -0.40. This suggests that PC2 captures a notable portion of the variance that is related to changes in drug concentration (as concentration increases, PC2 scores tend to decrease, or vice-versa).
#PC4: Shows a moderate positive correlation of 0.47. This also indicates PC4 is influenced by concentration.
#PC13: Shows a moderate positive correlation of 0.42.
#PC14: Shows a moderate negative correlation of -0.30.
#Other PCs generally have weaker correlations with
```



***

Such a correlation matrix can be visualized using a heatmap.

**Exercise 9: Create a heatmap of correlations between known factors and the selected PCs:**

<details>
  <summary><strong>Hint</strong></summary>

  `colorRamp2` can be used to generate a custom colour palette.

</details>

```{r heatmap}
# Create a colour scale from -1 to 1 for the heatmap
col_fun <- colorRamp2(c(-1, 0, 1), c("#008080", "white", "#b3002d"))

# Create a heatmap
Heatmap(
  t(cxy),         
  # Use the colour palette
  col = col_fun,  
  border = 'grey5',
  # Cluster the variables but not the PCs
  cluster_columns = FALSE,            
  show_row_dend = TRUE,             
  show_column_dend = FALSE,    
  # Set name of legend
  name = "Corr",      
  # Format text
  row_names_gp = gpar(fontsize = 8), 
  column_names_gp = gpar(fontsize = 8), 
  cell_fun = function(j, i, x, y, width, height, fill) {
    grid.rect(x, y, width, height, 
              gp = gpar(col = "white", lwd = 1, fill = NA))
    # Print correlation if it is above 0.4
    grid.text(ifelse(abs(t(cxy)[i,j]) > 0.4,
                     sprintf("%.2f", round(t(cxy)[i, j], 2)),
                     ""), 
              x, y, gp = gpar(fontsize = 8, col = "white"))
  }
)

#Concentration (conc_amt_numeric) is clearly linked to the variation captured by PC2 and PC4. This is often a desired outcome, as it suggests these PCs are reflecting biological responses or changes induced by the different compound concentrations.

# The PCA appears to have somewhat separated the variance driven by compound concentration from the variance driven by library size, as these factors correlate most strongly with different sets of PCs.

#You can infer that PC2 and PC4 are likely capturing variance related to the biological effects of the compound concentrations.

#PC1 (which explains the most variance overall) does not show a strong correlation with any of the three factors displayed in this heatmap. This suggests that the largest source of variation in your dataset might be driven by another factor not included in this specific correlation analysis (e.g., the difference between the two main compounds, Diisopropanolamine vs. L-Tyrosine, or other unmeasured batch effects or biological differences). You would typically explore this further by creating PCA plots (like in Exercise 10) and coloring the samples by other metadata variables (like compound_name or plate_ID).
```

***

## PCA plot

**Exercise 10: Make a plot of two important PCs against each other, coloured by a relevant factor:**

<details>
  <summary><strong>Hint</strong></summary>

  You can use different shapes to visualize multiple factors in the same plot.

</details>
```{r pca-plot}
metaData %>% 
  # Plot PC2 against PC3
  ggplot(aes(x = PC2, y = PC3, 
             # Coloured by compound class and with different shapes for each plate
             color = compound_class, shape=plate_ID)) +
  geom_point(size = 2) +
  # Set labels
  labs(x = paste0("PC2 (", round(100*var_explained[2,2], 2), "%)"), 
       y = paste0("PC3 (", round(100*var_explained[3,2], 2), "%)"), 
       color = "Class", shape = "Plate") +
  ggtitle("PCA plot") +
  theme_bw()

# The PCA plot effectively shows that the type of compound used is the biggest driver of gene expression differences in your data (captured by PC1). PC2 captures additional, smaller variations, potentially linked to dose responses. The controls group as expected.
```


***

# Advanced questions

Sometimes a PCA plot can highlight important clusters in your data. Gene loadings can be used to assess which gene's expression is driving these clusters.

**Exercise 11: Investigate a pattern in your data and identify what genes are responsible for it:**

```{r advanced-pca}

metaData %>% 
  # Convert concentration ID to a factor
  mutate(conc_ID = factor(conc_ID)) %>% 
  # Plot PC1 against PC2
  ggplot(aes(x = PC1, y = PC2, 
             shape = compound_ID, color = conc_ID)) +
  geom_point(size = 2) +
  labs(x = paste0("PC1 (", round(100*var_explained[1,2], 2), "%)"), 
       y = paste0("PC2 (", round(100*var_explained[2,2], 2), "%)"), 
       color = "Conc ID", shape = "Compound") +
  theme_bw()


#gene_name_from_pca (<chr> for character): This column contains the identifiers for your genes. These names come from the row names of your logcpmData_for_pca after it was transposed (so they were originally column names of t(logcpmData_for_pca))
#PC1 (<dbl> for double/numeric):This column shows the loading score for each gene on PC1.
#abs_loading (<dbl> for double/numeric):This column shows the absolute value of the PC1 loading score

# The genes listed at the top of this table are the ones whose expression patterns are most strongly associated with the variation explained by PC1.


```



***

# Session Info

**Exercise 12: Print your session info at the end of the script, knit the R markdown document, and push it to GitHub:**

```{r session-info}
library(sessioninfo)
session_info()
```


***

That is the end of the entire QC pipeline. Example answers will be available from the `BOO_template` GitHub on Tuesday. 

***
