---
title: "BOO 2025 - Example Analysis"
subtitle: "Script 3: Sample QC - Questions"
date: "`r Sys.Date()`" 
author: 
 Hala Saab
output:
  html_document:
    code_download: true
    theme: united
    highlight: tango
    df_print: paged
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

***

> In this script, you will perform sample-level quality control (QC), removing any poor quality samples and ensuring that experimental replicates are comparable to one another. 

***

# Setup

## Clean

As before, we perform several setup steps at the start of the script to ensure our work is reproducible and clear. 

**Exercise 1: Download the R markdown template and clean your environment:**

```{r clean}
# clean
 rm(list = ls())

```

***

## Set variables

**Exercise 2: Create the following objects in your R environment:**

* `root_dir` - project folder
* `count_path` - location of the `countData` object within the project folder
* `cpm_path` - location of the `cpmData` object
* `metadata_path` - location of the `metaData` object

* `count_store` - location to save the `countData` object within the project folder
* `cpm_store` - location to save the `cpmData` object
* `metadata_store` - location to save the filtered `metaData` after QC

* `count_threshold` - minimum library size required for samples (600,000; 20% of the target sequencing depth)
* `corr_threshold` - required correlation between replicates (0.9)

```{r set-variables}
#

count_path <- file.path("output/qc_countData.RData") 
cpm_path <- file.path("output/cpm_data.RData")      
metadata_path <- file.path("output/processed_metaData.RData") 

output_dir_script3 <- "output"
if (!dir.exists(output_dir_script3)) {
  dir.create(output_dir_script3)
  cat("Created directory:", output_dir_script3, "\n")
}

count_store <- file.path(output_dir_script3, "sampleQC_countData.RData")
cpm_store <- file.path(output_dir_script3, "sampleQC_cpmData.RData")
metadata_store <- file.path(output_dir_script3, "sampleQC_metaData.RData")

count_threshold <- 600000 # Minimum library size
corr_threshold <- 0.9     # Minimum replicate correlation

```

***

## Packages

Here, we load `tidyverse` and also a new package:

* `ggrepel` allows us labels in plots to "repel" each other and make visualizations clearer

**Exercise 3: Load `tidyverse` and `ggrepel` into your environment:**

```{r load-packages, warning=F, message=F}
# Load packages
library(tidyverse)
library(ggrepel) # For repelling text labels in ggplot
```

***

## Load data

**Exercise 4: Load the count data, CPM data, and metadata into your environment:**

<details>
  <summary><strong>Hint</strong></summary>

  Make sure these are the ones your saved at the end of the probe QC.

</details>

```{r load-data, warning=FALSE, message=FALSE}
# Load data saved from Script 2

path_to_cpmData <- "output/cpm_data.RData"
load("C:/Users/hala2/Universiteit Leiden/BOO 2025 - BOO CDS Giulia team - BOO CDS Giulia team/Students/Hala/Project/Hala-BOO/output/cpm_data.RData")

load("C:/Users/hala2/Universiteit Leiden/BOO 2025 - BOO CDS Giulia team - BOO CDS Giulia team/Students/Hala/Project/Hala-BOO/output/countData.Rdata")


load("C:/Users/hala2/Universiteit Leiden/BOO 2025 - BOO CDS Giulia team - BOO CDS Giulia team/Students/Hala/Project/Hala-BOO/output/metaData.Rdata")

```

***

# Library size

## Check

Before applying any filters, it is good to perform some checks.

**Exercise 5: Check that the column names of `countData` match the `sample_ID` order in `metaData`:**

```{r order-check}
#1. Get sample identifiers from the column names of countData.
countData_sample_colnames <- colnames(countData)

# 2. Get sample identifiers from the 'sample_ID' column of metaData.
metaData_sample_ids <- metaData$sample_ID

are_identical_and_in_order <- all(countData_sample_colnames == metaData_sample_ids)

print (are_identical_and_in_order)

# Yes, there are identical
```

***

## Calculate

**Exercise 6: Now that we have removed unreliable and lowly expressed probes from `countData`, recalculate and save a new `lib_size` in the metadata:**

```{r calculate-lib}
# Calculate new library sizes from the current (probe-QC'd) countData
recalculated_library_sizes <- colSums(countData)


#Create a data frame from these new library sizes for merging.
new_lib_size_df <- data.frame(
  sample_ID = names(recalculated_library_sizes),
  lib_size_recalculated = recalculated_library_sizes,
  row.names = NULL # Ensure default row names for this temporary df
)
# 3. Merge the new library sizes into 'metaData'.
if ("lib_size" %in% colnames(metaData)) {
  metaData <- metaData %>% select(-lib_size)
  cat("Removed existing 'lib_size' column from 'metaData'.\n")
}

metaData <- metaData %>% 
  select(-any_of(c("lib_size_recalculated", "lib_size.x", "lib_size.y")))

metaData <- metaData %>%
  left_join(new_lib_size_df, by = "sample_ID") %>%
  rename(lib_size = lib_size_recalculated)

print(head(metaData %>% select(sample_ID, lib_size, any_of(c("low_reads_flag", "low_corr_flag")))))

#lib_size: This column now contains the newly recalculated library sizes. These values were derived by summing up all the counts for each sample in your current countData object
```

***

## Distribution

**Exercise 7: Make a histogram of `lib_size`. What range of values does this variable take and is this reasonable for a TempO-Seq experiment?**

```{r lib-histogram}

print(summary(metaData$lib_size))

lib_size_histogram <- ggplot(metaData, aes(x = lib_size)) +
  geom_histogram(
    binwidth = 200000, 
    fill = "steelblue", 
    color = "black",    
    alpha = 0.7       
  ) +
  scale_x_continuous(
    labels = scales::comma,
    n.breaks = 8         
  ) + 
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) + 
  labs(
    title = "Distribution of Sample Library Sizes (after Probe QC)",
    x = "Library Size (Total Counts per Sample)",
    y = "Number of Samples (Frequency)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
print(lib_size_histogram)

#The distribution appears roughly unimodal, with the main peak centered around 3.5 to 4.0 million reads. This aligns well with your calculated median (~3.66M) and mean (~3.64M).
#Based on mu summary statistics, the library sizes range from a minimum of 1,255,337 to a maximum of 5,244,421. The histogram visually confirms this spread, with bars covering this range.
#Yes, this range and distribution are generally reasonable and look quite good for a TempO-Seq experiment.Sufficient Depth, Above Minimum Thresholds and No Extreme Outliers.
```

***

## Flag

Samples whose library size is below 20% of the targeted sequencing depth (`corr_threshold`; 600,000) should be flagged as having low reads.

**Exercise 8: Create a flag in `metaData` describing if samples have low reads or not:**

```{r lib-flag}
# 1. Create the 'low_reads_flag' in metaData.
if ("low_reads_flag" %in% colnames(metaData)) {
  metaData <- metaData %>% select(-low_reads_flag)
}
metaData <- metaData %>% 
  select(-any_of(c("low_reads_flag.x", "low_reads_flag.y")))

metaData <- metaData %>%
  mutate(
    low_reads_flag = ifelse(lib_size < count_threshold, TRUE, FALSE)
  )
# 2. Display a summary of the new flag.
print(table(metaData$low_reads_flag, useNA = "ifany"))

#This output means that all 60 samples in your metaData have a low_reads_flag of FALSE. Therefore, no samples were flagged as having low reads because all of them had a library size greater than or equal to the 600,000 count threshold.
```

***

## Plot

It is good to visualize the library size for each sample, grouped by compound ID. This shows us whether samples are well above the threshold, and allows us to inspect the data more carefully.

**Exercise 9: Create a boxplot for the library sizes of each compound (including DMSO) and describe any patterns you identify:**

<details>
  <summary><strong>Hint</strong></summary>

  You can colour your boxplots by concentration to visualize patterns more clearly.

</details>

```{r lib-boxplot}


plot_data_boxplot <- metaData %>%
  mutate(
    conc_factor = factor(conc_amt), # Treat concentration as a categorical variable for fill
    low_reads_flag = factor(low_reads_flag, levels = c(TRUE, FALSE), labels = c("Low Reads", "OK Reads"))
  )

print(table(plot_data_boxplot$low_reads_flag, useNA = "ifany"))

# Create the boxplot
lib_size_boxplot <- ggplot(plot_data_boxplot, aes(x = compound_name, y = lib_size)) +
  geom_boxplot(
    aes(fill = conc_factor), 
    outlier.shape = NA, # We'll use jitter to show all points, so hide default boxplot outliers
    alpha = 0.7,        # Transparency for the box fill
    notch = FALSE       # Optional: set to TRUE for notched boxplots
  ) +
  geom_jitter(
    aes(color = low_reads_flag), 
    width = 0.25,       # Amount of horizontal jitter
    alpha = 0.8,        # Transparency of points
    size = 1            # Size of points
  ) +
  geom_hline(
    yintercept = count_threshold, 
    linetype = "dashed", 
    color = "red", 
    linewidth = 1
  ) +
  annotate(
    "text", 
    x = Inf, # Position to the far right
    y = count_threshold, 
    label = paste("Threshold =", scales::comma(count_threshold)), 
    hjust = 1.05, # Adjust to be just right of the plot edge
    vjust = -0.5, # Adjust to be slightly above the line
    color = "red", 
    size = 2
  ) +
  scale_y_continuous(labels = scales::comma) + # Format y-axis with commas
  scale_fill_viridis_d(option = "D", name = "Concentration Amount") + # Colorblind-friendly discrete palette for fill
  scale_color_manual(
    name = "Library Size Status",
    values = c("Low Reads" = "orange", "OK Reads" = "black"), # Colors for jitter points
    drop = FALSE # Ensures all legend items appear even if no data for one
  ) +
  labs(
    title = "Library Size Distribution by Compound",
    subtitle = "Points show individual samples, colored by library size status relative to threshold.",
    x = "Compound Name",
    y = "Library Size (Total Counts per Sample)"
  ) +
  theme_minimal(base_size = 8) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 7),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 7), # Angle x-axis labels
    legend.position = "top"
  )


print(lib_size_boxplot)





# Controls (DMSO, Medium): Show consistent and high library sizes, indicating good quality.
#Diisopropanolamine: Library sizes are fairly consistent across its different concentrations.
#L-Tyrosine: Shows more variability in library sizes across its concentrations compared to other groups.
#Overall: No obvious trend of library size changing systematically with drug concentration is visible. The data generally indicates robust sequencing depth across all samples.


```


***

# Replicate correlation

## log2CPM

The replicate correlation filter aims to remove any outlying replicates, with maximum pairwise correlations below the `corr_threshold` (set to 0.9). We usually perform this correlation analysis on the log2CPM values to ensure highly expressed genes do not have undue influence on the correlation values. A value of 1 is added to the CPM, to prevent issues arising from the fact that `log2(0)` is `-Inf`. 

**Exercise 10: Calculate and store the log2(CPM + 1) values in a `logcpmData` object:**

```{r log2cpm}
#Calculate log2(CPM + 1)
logcpmData <- log2(cpmData + 1)

```

***

## Pairwise correlations

In order to calculate pairwise correlations, each sample needs to be compared to the other replicates in its experimental group. We can do this by looping through `mean_ID`.

**Exercise 11: Calculate the pairwise replicate correlations for this data:**

<details>
  <summary><strong>Hint</strong></summary>

  The correlation can be calculated using `cor(cpmDataReps[,j], cpmDataReps[,k])` within an appropriate loop.

</details>

```{r pairwise-corr}

# Display the head of the new logcpmData object.
replicateFilterOutput <- data.frame()

# For each mean ID (experimental condition)
for(i in unique(metaData$mean_ID)){
  # Subset the meta data to keep only samples from this experiment
  metaDataReps <- metaData %>% 
    filter(mean_ID == i)
  
  # Subset the log2 CPM values to keep only these samples
  cpmDataReps <- logcpmData[, metaDataReps$sample_ID] 
  
  # Loop through each column in the CPM data  
  for(j in 1:ncol(cpmDataReps)){
    # In a pairwise fashion
    for(k in 1:ncol(cpmDataReps)){
      # Save the position in the loops that you are in
      sample_A <- colnames(cpmDataReps)[j]
      sample_B <- colnames(cpmDataReps)[k]
      
      # Don't calculate pairwise correlations between identical samples (since it will be 1)  
      if(sample_A != sample_B){
        # Calculate pairwise correlation values
        r2 <- cor(cpmDataReps[,j], cpmDataReps[,k])
        
        # Update the filter output data frame
        replicateFilterOutput <- rbind(
          replicateFilterOutput, 
          data.frame(mean_ID = i, 
                     sample_A = sample_A,
                     sample_B = sample_B,
                     r2 = r2))
      }
    }
  }
}
head(replicateFilterOutput)


```

***

## Maximum

Each sample is judged by the best pairwise correlation it can achieve. If this is below `corr_threshold`, the sample should be flagged.

**Exercise 12: Calculate the `max_r2` for each sample and add it to the `replicateFilterOutput`:**

```{r max-r2}
replicateFilterOutput <- replicateFilterOutput %>% 
  # Separate sample name into compound and concentration using the underscore
  separate(sample_A, 
           into = c("Compound", "Conc_ID", NA, NA), 
           remove = F, 
           sep = "_") %>% 
  # If the compound is DMSO then keep only the first 5 letters (else we get DMSOHigh1 and DMSOHigh2 :D)
  mutate(Compound = ifelse(grepl("DMSO", Compound), substr(Compound,1,5), Compound)) %>% 
  # Group by sample
  group_by(sample_A) %>%
  # Save the maximum pairwise correlation for that sample
  mutate(max_r2 = max(r2, na.rm = T)) %>% 
  ungroup()

# Inspect output
summary(replicateFilterOutput$max_r2)



```

***

## Plot

**Exercise 13: Visualize the pairwise replicate correlations for each experimental conditions. Describe what you observe:**

```{r corr-boxplot}

replicateFilterOutput %>% 
  # Plot the sample ID against the pairwise correlation
  ggplot(aes(x = sample_A, y = r2)) +
  # Draw a boxplot of the pairwise correlation distribution
  geom_boxplot(color = "grey80") +
  # With points for the actual values
  geom_point(color = "grey60", size = 0.5) +
  # Highlight the maximum pairwise correlation for each sample
  geom_point(aes(y = max_r2, color = Conc_ID), 
             size = 1.5) +
  # Draw a line for the filter threshold
  geom_hline(aes(yintercept = corr_threshold), 
             color = "grey60", linetype = "dashed") +
  ylab("") + xlab("Sample ID") + ggtitle("Replicate correlations") +
  theme_bw() +
  # Do not print sample names
  theme(axis.text.x = element_blank()) +
  # Make a different plot for each compound, allowing the x-axis to change for different samples
  facet_wrap(~Compound, scales='free_x', nrow=2)


#The boxplot shows that replicate correlations (max_r2) are very high across almost all samples and conditions, with most values well above the 0.9 threshold (red dashed line). This indicates good reproducibility.

#Controls (DMSOHigh_P1, DMSOLow_P1, Medium_P6) show tight, consistently high correlations.
#Treatments (Diisopropanolamine and L-Tyrosine conditions) also show very good correlations, generally comparable to controls, with low variability within most groups.

```

***

## Flag

**Exercise 14: Flag any samples that did not pass the replicate correlation filter in the `metaData`:**

<details>
  <summary><strong>Hint</strong></summary>

  You can merge the replicate correlation filter output with the metaData to create a `max_r2` column after some processing.

</details>

```{r corr-flag}
# Make a data frame of sample IDs and max r2
replicateFilterMerge <- replicateFilterOutput %>% 
  select(sample_ID = sample_A, max_r2) %>% 
  distinct()

# Merge with meta data
metaData <- left_join(metaData, replicateFilterMerge, 
                      by = "sample_ID") %>% 
  mutate(flagLowCorr = ifelse(max_r2 <= corr_threshold, T, F))

table(metaData$flagLowCorr)


# This output indicates that all 60 samples in your dataset have a low_corr_flag of FALSE. This means that no samples were flagged as having a low replicate correlation based on your criterion (max_r2 < corr_threshold, where corr_threshold is 0.9).

#This aligns with the visual interpretation of the boxplot from Exercise 13, where all samples appeared to have max_r2 values at or above the 0.9 threshold. It's a good sign, suggesting strong reproducibility among your replicates.
```

```{r}



metaData %>% 
  filter(flagLowCorr)
```


***

# Advanced questions

If you would like a bit more of a challenge, here are a few extra questions relating to the two sample QC steps above. However, you can also skip these, save your data, and move on to the PCA.

## Library size

**Exercise 14: What are the benefits of a sample having a higher library size and does this benefit apply to some genes more than others?**

```{r read-depth}
#

```

***

## Replicate correlation

Instead of looking at pairwise correlations, another way of measuring how good a replicate is is by comparing it to the average for that experimental condition. 

**Exercise 15: Calculate replicate correlation in this way and see if it alters the results of this filter. What is one benefit and downside of assessing replicate correlation in this manner?**

```{r mean-corr}
#

```

***

# Save

**Exercise 16: Remove samples that did not pass the sample QC steps from your data:**

<details>
  <summary><strong>Hint</strong></summary>

  Don't forget to also subset the count and CPM data.

</details>

```{r any-flag}
# Subset the metadata to keep only high quality samples
metaData_qc <- metaData %>% 
  filter(!low_reads_flag & !flagLowCorr)

# Subset the count and CPM data
cpmData_qc <- cpmData[ , metaData$sample_ID]
countData_qc <- countData[ , metaData$sample_ID]

# Check dimensions
dim(metaData)


dim(countData)
dim(cpmData)



```

***

## Save

**Exercise 17: Save the updated data:**

```{r save-metadata}


write.csv(countData_qc,"output/count_store.csv")
write.csv(metaData_qc,"output/metadata_store.csv")

write.csv(cpmData_qc,"output/cpm_store.csv")

```

```{r}

write_csv(countData_qc,"output/count_store.csv")
write_csv(metaData_qc,"output/metadata_store.csv")

write_csv(cpmData_qc,"output/cpm_store.csv")

```



***

# Session Info

**Exercise 18: Print your session info at the end of the script, knit the R markdown document, and push it to GitHub:**

```{r session-info}
library(sessioninfo)
session_info()

```

***

That is the end of the Sample QC. Example answers will be available from the `BOO_template` GitHub on Tuesday. 

Next, please move on to the PCA using `04_PCA_Outline.Rmd`.

***

